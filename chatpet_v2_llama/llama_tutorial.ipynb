{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA Loading Tutorial\n",
    "- Environment\n",
    "    - torch\n",
    "    - fairscale\n",
    "    - fire\n",
    "    - sentencepiece==0.1.97\n",
    "    - hiq-python (if hiq is needed)\n",
    "- model and ckpt\n",
    "    - follow the command bellow (or Just using url to get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Kasy/kasy_files/iArt.ai_Lab_Tech_Stack/model_deployment\n",
      "/home/Kasy/kasy_files/iArt.ai_Lab_Tech_Stack/model_deployment/llama\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the directory containing the current script\n",
    "current_dir = os.path.dirname(os.path.abspath('./'))\n",
    "print(current_dir)\n",
    "# Construct the path to the 'llama' folder\n",
    "module_path = os.path.join(current_dir, 'llama')\n",
    "print(module_path)\n",
    "# Add the 'llama' folder path to the Python module search path\n",
    "sys.path.append(module_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama import LLaMA_Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Model\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
    "- wget https://agi.gpt4.org/llama/LLaMA/7B/params.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim :  4096\n",
      "multiple_of :  256\n",
      "n_heads :  32\n",
      "n_layers :  32\n",
      "norm_eps :  1e-06\n",
      "vocab_size :  -1\n"
     ]
    }
   ],
   "source": [
    "# The LLaMA Construction\n",
    "from llama import ModelArgs, LLaMA_Transformer\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "ckpt_dir = './llama_model_config/7B/'\n",
    "\n",
    "with open(Path(ckpt_dir)/ \"params.json\", 'r') as f:\n",
    "    params = json.loads(f.read())\n",
    "\n",
    "for para in params:\n",
    "    print(para, ': ', params[para])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama import LLaMA_Tokenizer\n",
    "\n",
    "tokenizer_pth = './llama_model_config/tokenizer.model'\n",
    "llama_tokenizer = LLaMA_Tokenizer(\n",
    "    model_path = tokenizer_pth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 269, 893]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function Test\n",
    "llama_tokenizer.encode(\n",
    "    s = 'sss',\n",
    "    bos=True, \n",
    "    eos=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of  32000\n",
      "the token_id of begining of the sentence is: 1\n",
      "the token_id of end of the sentence is: 2\n",
      "the token_id of padding is: -1\n"
     ]
    }
   ],
   "source": [
    "# Some Information of the LLaMA Tokenizer\n",
    "print('the number of ',llama_tokenizer.n_words)\n",
    "print('the token_id of begining of the sentence is:', llama_tokenizer.bos_id)\n",
    "print('the token_id of end of the sentence is:', llama_tokenizer.eos_id)\n",
    "print('the token_id of padding is:', llama_tokenizer.pad_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization in Generation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a Prompt\n",
    "prompts = [\n",
    "        # Text Generation Test\n",
    "        \"Said you love me, but I don't \",\n",
    "        \"Building a website can be done in 10 simple steps: (this website should be able to have a calculator function)\\n\",\n",
    "        \"Expand on this sentence: <the meaning of life is> \"\n",
    "        # Instruction Prompting (Expand later)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size = 32  # Assume a max\n",
    "\n",
    "# Count the number of prompts, and make sure it's not exceed max batch size\n",
    "bsz = len(prompts)  # batch size\n",
    "assert bsz <= max_batch_size, f'the prompts exceeds the max number for you prompt {bsz} but max is {max_batch_size}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "13\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# Tokenization to each prompt using the LLaMA_Tokenizer\n",
    "prompt_tokens = [llama_tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "\n",
    "min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "print(len(prompt_tokens))\n",
    "print(min_prompt_size)\n",
    "print(max_prompt_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_gen_len = 256\n",
    "max_seq_len = 2048  # Which indicate the max prompt length is 2048\n",
    "\n",
    "# max_gen_len + max_prompt_size as total length but without exceeding max_seq_len\n",
    "total_len = min(max_seq_len, max_gen_len + max_prompt_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Fullfil the tensor with pad tokens with the dimension of (batch_size, total_len)\n",
    "tokens = torch.full((bsz, total_len), llama_tokenizer.pad_id).cuda().long()\n",
    "\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    print(k)\n",
    "    print(t)\n",
    "    print(len(t))\n",
    "    tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mask info, which is the position of the prompt tokens\n",
    "# Used in the generation process if to replace the prompt tokens with the generated tokens\n",
    "input_text_mask = tokens != llama_tokenizer.pad_id\n",
    "\n",
    "print(input_text_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loading\n",
      "Loaded in 9.43 seconds\n",
      "I believe the meaning of life is to find happiness and be satisfied with what you have.\n",
      "People have different definitions of happiness. Some people feel that if they could only win the lottery, they would be happy. Some people feel that if they could only get that promotion, they would be happy. Some people feel that if they could only be the top scorer in a game, they would be happy.\n",
      "If you do not know what happiness is, I suggest you ask a psychologist. A psychologist has studied the subject of happiness and he or she knows what happiness is. A psychologist has a Ph.D. in psychology and is an expert on the subject of happiness. A psychologist knows how to make people happy.\n",
      "Although you might know what happiness is, you might have forgotten it. If that is the case, I suggest you consult a psychologist. A psychologist can make you happy again. A psychologist can help you discover your happiness and how to be happy.\n",
      "Happiness is a big word. Happiness is a nice word. Happiness is a beautiful word.\n",
      "I believe that the meaning of life is to find happiness and be satisfied with what you have.\n",
      "People have different definitions of happiness. Some people feel\n",
      "\n",
      "==================================\n",
      "\n",
      "Simply put, the theory of relativity states that 1) there is no absolute time or space and 2) the speed of light in a vacuum is the fastest speed possible. There are two key principles in relativity:\n",
      "(1) The laws of physics are the same in all inertial reference frames.\n",
      "(2) The speed of light is constant in all inertial reference frames.\n",
      "The second of these principles has allowed us to prove the first.\n",
      "Before Einstein, scientists believed that the speed of light was constant in all frames, but that the speed of light was not constant. This was called the constancy of the speed of light hypothesis. In the late 19th century, scientists such as Michelson and Morley and Lorentz had set up experiments to test this hypothesis.\n",
      "For example, when Michelson and Morley set up their Michelson-Morley interferometer, they expected that the light would take a different path depending on whether it was moving at the same speed as the Earth or at a different speed. They found that it didn't, but the constancy of the speed of light hypothesis said that it would.\n",
      "Why didn't the constancy of the speed of light hypothesis work? Because it was wrong\n",
      "\n",
      "==================================\n",
      "\n",
      "Building a website can be done in 10 simple steps:\n",
      "1. Decide what you need\n",
      "What is it that you need to do? Do you want people to buy a product, do you want to have a blog or do you want to provide a service? These are all different types of websites. The type of website you need will have a large impact on how you design your website.\n",
      "2. Do you want a responsive website?\n",
      "Some websites are better suited for a mobile or tablet. If you are planning to have a mobile site, then it is recommended to have a responsive website. This allows your website to be displayed on any device.\n",
      "3. What is your target audience?\n",
      "Having a clear idea of who your target audience is can help you design a website that will be targeted to them. Also, you will be able to know how to market your website and get people to your website.\n",
      "4. What are you going to use?\n",
      "There are many different web designing platforms. Some are free and others cost money. There are pros and cons to both types of websites.\n",
      "5. Decide on the look and feel\n",
      "What is your website going to look like? What colors are you going to use? Is there a certain logo or icon that you\n",
      "\n",
      "==================================\n",
      "\n",
      "Tweet: \"I hate it when my phone battery dies.\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"My day has been ðŸ‘\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"This is the link to the article\"\n",
      "Sentiment: Neutral\n",
      "###\n",
      "Tweet: \"This new music video was incredibile\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"My heart is broken\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"I have some great news\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"My favorite band just announced a new album\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"That food was so good\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"My company just moved to a new building\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I just ate the best lunch ever\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"We are at 87% to our goal\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"I just got an awesome new job\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"My favorite sports team just won the championship\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"My favorite sports team just lost\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"My favorite sports team just won\"\n",
      "Sentiment: Positive\n",
      "##\n",
      "\n",
      "==================================\n",
      "\n",
      "Translate English to French:\n",
      "\n",
      "sea otter => loutre de mer\n",
      "\n",
      "peppermint => menthe poivrÃ©e\n",
      "\n",
      "plush girafe => girafe peluche\n",
      "\n",
      "cheese => fromage\n",
      "\n",
      "Tell me what you need, and I'll do my best to give you the best answers.\n",
      "\n",
      "Answer: You will find many translators on the web. For example: http://www.french-translator.com/translator.html\n",
      "\n",
      "You can also use the Wikipedia as a guide to understand the french meaning.\n",
      "\n",
      "Comment: Your answer could be improved with additional supporting information. Please [edit] to add further details, such as citations or documentation, so that others can confirm that your answer is correct. You can find more information on how to write good answers [in the help center](/help/how-to-answer).\n",
      "\n",
      "Answer: There is no automatic tool to translate English to French. There are a couple of tools on the web, but I do not know if you will find anything to help you.\n",
      "\n",
      "You can find a translation in a dictionary, a bilingual dictionary, a French to English one, if you find a French definition for the term.\n",
      "\n",
      "Then you can try to match your translation with the French definition.\n",
      "\n",
      "Answer: You can use Google Translate to translate words from English to French. But it is important to note that\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the command on At Least 16G RAM, with at least 24G GPU RAM\n",
    "!torchrun --nproc_per_node 1 gen.py --ckpt_dir ./llama_model_config/7B --tokenizer_path ./llama_model_config/tokenizer.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
